{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "961xR8LdcJQY"
   },
   "outputs": [],
   "source": [
    "# Small LLM / Notebook created by Javier Ideami (ideami.com)\n",
    "# Typical LLMs need many GPUs and millions of dollars to be trained\n",
    "# This code trains a small LLM with a single GPU and little GPU memory \n",
    "# Of course results are not like a chatGPT, but they are good enough to see how the LLM trains to go\n",
    "# from random combinations of letters to actual words and phrases that are sometimes decently coherent\n",
    "# GPT3 has 175 Billion parameters. GPT4 has many, many more.\n",
    "# This model has only 19 Million parameters with its default settings. That's why its perfect for learning \n",
    "# and experimenting\n",
    "\n",
    "# Official notebook #vj30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For GOOGLE COLAB and similar platform Users:\n",
    "#### Make sure to select a GPU in the online platform. Don't run this code with a CPU (it will be too slow)\n",
    "\n",
    "# If you are running this code locally, your GPU should be selected automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1tUgAJccK6D",
    "outputId": "dbabcd5d-ad95-4518-a0f8-e04fddbce82c"
   },
   "outputs": [],
   "source": [
    "# uncomment and run the following installation lines ONLY if you havent installed these libraries already outside of the notebook\n",
    "#!pip install ipdb -q\n",
    "#!pip install tqdm -q\n",
    "#!pip install sentencepiece -q\n",
    "#!pip install wandb -q\n",
    "\n",
    "# And if you are not in Google Colab and you didn't yet install Pytorch, make sure to do it:\n",
    "# find the ideal pytorch installation command at https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 11 11:18:22 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.77.01              Driver Version: 566.36         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080 Ti     On  |   00000000:01:00.0  On |                  N/A |\n",
      "| 53%   41C    P8             29W /  350W |    4562MiB /  12288MiB |      6%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     25254      C   /python3.12                                 N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# You can use this command to view information about your GPU and the amount of free memory it has\n",
    "# Make sure that you have at last 4GB of free GPU memory to do this course\n",
    "!nvidia-smi \n",
    "# If you are using Google Colab or a similar online platform, make sure to select a GPU in the menus\n",
    "# In Google colab, at the moment the option is within the Runtime menus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6VnNqwkhiU3n"
   },
   "outputs": [],
   "source": [
    "### Import necessary libraries\n",
    "\n",
    "import os, sys\n",
    "import ipdb # for debugging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import platform, shutil # detect platform type\n",
    "import requests, zipfile, io \n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import sentencepiece as spm # For the tokenizer\n",
    "\n",
    "# These lines improve performance for Ampere Architecture (e.g: A100s)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "# Empty GPU cache memory\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "G5q26l98govJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you seem to have already downloaded the files. If you wish to re-download them, delete the encoded_data.pt file\n"
     ]
    }
   ],
   "source": [
    "# Download necessary files and create necessary folders\n",
    "# wiki.txt - dataset: a tiny segment of the English Wikipedia\n",
    "# wiki_tokenizer.model: trained tokenizer file (in another notebook I show you how to produce this file)\n",
    "# wiki_tokenizer.vocab: trained tokenizer file (in another notebook I show you how to produce this file)\n",
    "# encoded_data.pt (dataset tokenized with the tokenizer)\n",
    "# I will explain how to produce encoded_data.pt - because it takes quite a bit to process, it's nice to have it in advance\n",
    "\n",
    "# NOTE: Downloading will take a while, be patient. You can refresh your folder from time to time to see when the files\n",
    "# have been created. If you have any problems downloading the files with this code, I have also added llm_train.zip\n",
    "# to the downloadable resources of this lecture (however, best option is to use this code, because then you don't need\n",
    "# to upload the files or do anything else)\n",
    "\n",
    "files_url = \"https://ideami.com/llm_train\"\n",
    "\n",
    "# Downloading proceeds if we detect that one of the key files to download is not present\n",
    "if not os.path.exists(f\"encoded_data.pt\"):\n",
    "    print(\"Downloading files using Python\")\n",
    "    response = requests.get(files_url)\n",
    "    zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\n",
    "else:\n",
    "    print(\"you seem to have already downloaded the files. If you wish to re-download them, delete the encoded_data.pt file\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fre7fXD0fVD9",
    "outputId": "04d590af-d8cc-4e93-fd10-60b97fc473d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: You will be using:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Set main parameters\n",
    "\n",
    "# ARCHITECTURE PARAMETERS\n",
    "batch_size= 16 # How many samples do we train at once (set as needed, typical range 8 to 128)\n",
    "              # 8 is good for a GPU with 4GB of memory, 128 is good for a GPU with 24GB of memory\n",
    "context=512 # Sequence length used for training, 512 is a good compromise for our level of resources\n",
    "embed_size=384 # Embedding size\n",
    "n_layers = 7 # Number of transformer layers\n",
    "n_heads = 7 # Number of heads within each layer\n",
    "BIAS = True # Do we want Bias parameters?\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "lr = 3e-4 # Initial learning rate\n",
    "dropout=0.05 # Dropout percentage\n",
    "weight_decay = 0.01 # Weight decay regularizer\n",
    "grad_clip = 1.0 # Gradient clipping to prevent gradient explosion\n",
    "\n",
    "# TRAINING parameters\n",
    "train_iters = 100000 # Maximum number of training iterations\n",
    "eval_interval=50 # How often do we evaluate the performance?\n",
    "eval_iters=3 # Number of iterations while we evaluate performance\n",
    "compile = False # Compile will accelerate performance in compatible systems\n",
    "load_pretrained = True # Do we want to load a pretrained model to continue training?\n",
    "\n",
    "checkpoint_dir = 'models/'  # Where do we store checkpoints?\n",
    "\n",
    "checkpoint_fn = \"latest.pt\" \n",
    "# Name of checkpoint file to be saved during training\n",
    "\n",
    "checkpoint_load_fn = \"latest.pt\" \n",
    "# Name of checkpoint file to be loaded when load_pretrained is True\n",
    "# You can load llm2.pt to experiment with a checkpoint that already reached 2.31 of loss\n",
    "\n",
    "dtype = torch.bfloat16 # our target internal data type\n",
    "\n",
    "# MODE\n",
    "# Do we want to run the model in inference mode?\n",
    "inference=True \n",
    "\n",
    "# DEVICE - Sets device to GPU or CPU (use GPU always)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device: You will be using: \",device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "0Z_omi-4fW0s",
    "outputId": "3f8ecc39-b72d-4825-a78a-2705f66a7210"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mharikrishna1912\u001b[0m (\u001b[33mharikrishna1912-regology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hari/works/llm_mastery/wandb/run-20241211_111824-hqbwss7j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harikrishna1912-regology/test/runs/hqbwss7j' target=\"_blank\">test-run2024_12_11_11_18_23</a></strong> to <a href='https://wandb.ai/harikrishna1912-regology/test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harikrishna1912-regology/test' target=\"_blank\">https://wandb.ai/harikrishna1912-regology/test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harikrishna1912-regology/test/runs/hqbwss7j' target=\"_blank\">https://wandb.ai/harikrishna1912-regology/test/runs/hqbwss7j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LOGGING parameters\n",
    "# When you run this cell, it will ask you to enter your Wandb API Key, which you\n",
    "# can find at your account on https://wandb.ai/settings#api\n",
    "wandb_log = True\n",
    "wandb_project = \"test\"\n",
    "wandb_run_name = \"test-run\" + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)\n",
    "\n",
    "# The first time you run this logging code set to True, the weights and biases library\n",
    "# will ask you for an API key. You can follow the instructions in the video, or you can\n",
    "# also simply click on a link that should appear when you run this cell, pointing to this\n",
    "# address: https://wandb.ai/authorize  \n",
    "# Going to that address will allow you to quickly get an API key as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPrBqt7NhwnZ",
    "outputId": "85aad864-9cd1-4f74-fa76-63a24988a20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that was used to represent a team in an old TV show, The A-Team. A capital a is written \"A\". Use a capital A at the start of a sentence if writing.\n",
      "\n",
      "A is also a musical note, sometimes referred to as \"La\".\n",
      "\n",
      "The letter 'A' was in the Phoenician alphabet's aleph. This symbol came from a simple picture of an ox head. \n",
      "\n",
      "This Phoenician letter helped make the basic blocks of later types of the letter. The Greeks later modified this letter and used it as their letter alpha. The Greek alphabet was use\n"
     ]
    }
   ],
   "source": [
    "with open('wiki.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "\n",
    "print(text[10000:10500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDNcMXo_fals",
    "outputId": "8381a7a5-047a-425e-ece7-d958b18721e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size: 4096\n",
      "Encoding Decoding functions ready\n"
     ]
    }
   ],
   "source": [
    "# SENTENCEPIECE TOKENIZER\n",
    "\n",
    "# Load trained tokenizer\n",
    "# Make sure that \" model_file = \" is pointing to the right file\n",
    "sp = spm.SentencePieceProcessor(model_file='wiki_tokenizer.model')\n",
    "\n",
    "# Get the vocabulary size of our tokenizer\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(f\"Tokenizer vocab_size: {vocab_size}\")\n",
    "\n",
    "# Create the encoding and decoding tokenizer functions\n",
    "encode = lambda s: sp.Encode(s)\n",
    "decode = lambda l: sp.Decode(l)\n",
    "\n",
    "# Test that encoding and decoding are working well\n",
    "print(decode(encode(\"Encoding Decoding functions ready\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tA2mDSq_fhwC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved encoded data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31093/795341420.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load('encoded_data.pt')\n"
     ]
    }
   ],
   "source": [
    "# Tokenization of the dataset\n",
    "if os.path.exists(f\"encoded_data.pt\"):\n",
    "    # Load encoded data if you already saved it previously\n",
    "    print(\"Loading saved encoded data\")\n",
    "    data = torch.load('encoded_data.pt')\n",
    "else:\n",
    "    # If you still didn't encode and save the encoding, do it here\n",
    "    print(\"Encoding data\")\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    torch.save(data, 'encoded_data.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4B1cPQGnJM0",
    "outputId": "9b5d5e9d-db9a-4411-ef20-2177f99bf469"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 59.21 Million | Training: 53.29 Million | Validation: 5.92 Million\n"
     ]
    }
   ],
   "source": [
    "data_size=len(data) # Get the size of the dataset\n",
    "\n",
    "spl = int(0.9*data_size) # set the split at 90%-10%\n",
    "train_data=data[:spl] # training data will be 90% of the dataset\n",
    "val_data=data[spl:] # validation data will be 10% of the dataset\n",
    "print(f'Total data: {data_size/1e6:.2f} Million | Training: {len(train_data)/1e6:.2f} Million | Validation: {len(val_data)/1e6:.2f} Million')\n",
    "\n",
    "# data[:30] : shows the first 30 token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1EGi6Aevnjtp"
   },
   "outputs": [],
   "source": [
    "############## HELPER FUNCTIONS ###########################\n",
    "\n",
    "# Return a batch of either training or evaluation data\n",
    "def get_batch(split):\n",
    "    # BS = Batch Size / SL = Sequence Length or context length\n",
    "    data = train_data if split==\"train\" else val_data # Select the split\n",
    "    inds = torch.randint(len(data)-context, (batch_size,)) # (BS)\n",
    "    x = torch.stack([data[i: i+context] for i in inds]) # (BS,SL)\n",
    "    y = torch.stack([data[i+1: i+context+1] for i in inds]) # (BS,SL)\n",
    "\n",
    "    # Examples of what it returns\n",
    "    # # First 10 elements of first batch of inputs and labels\n",
    "    #x[0][:10] -> tensor([ 664,  278, 4031, 4056, 4065, 4062, 4062, 4051, 13, 13])\n",
    "    #y[0][:10] -> tensor([ 278, 4031, 4056, 4065, 4062, 4062, 4051,   13, 13, 4066])\n",
    "\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dusI66zcouBq",
    "outputId": "46d934e5-78c5-45a7-ea83-120ac72671ff"
   },
   "outputs": [],
   "source": [
    "# Uncomment to test your get_batch function\n",
    "#x,y=get_batch(\"train\")\n",
    "#print(f\"x.shape: {x.shape}\")\n",
    "#print(f\"y.shape: {y.shape}\")\n",
    "#print(x[0][:10])\n",
    "#print(y[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "34nHc2eJtH17"
   },
   "outputs": [],
   "source": [
    "# Head Attention Layer\n",
    "# Detects and reinforces patterns in relationships between members of sequence\n",
    "class Head(nn.Module):\n",
    "    # BS = Batch Size / SL = Sequence Length or context length\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.queries= nn.Linear(embed_size, head_size, bias=BIAS) # Query Projection (embed_dim, head_size) (384, 54)\n",
    "        self.keys= nn.Linear(embed_size, head_size, bias=BIAS) # Key Projection (384, 54)\n",
    "        self.values= nn.Linear(embed_size, head_size, bias=BIAS) # Value Projection (384, 54)\n",
    "        # We declare a triangular matrix that we will use to mask future tokens from the current position\n",
    "        # self.tril contains 0s in upper triangle and 1s in lower triangle + diagonal\n",
    "        self.register_buffer('tril',torch.tril(torch.ones(context,context))) # self.tril - (SL,SL)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        BS,SL, VS = x.shape\n",
    "        q=self.queries(x) # (BS,SL,54)  54 is the head_size\n",
    "        k=self.keys(x) # (BS,SL,54)\n",
    "        v=self.values(x) # (BS,SL,54)\n",
    "\n",
    "        # Calculate square attention weights matrix with dot product of q and k, and normalize\n",
    "        attn_w = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (BS, SL, SL)\n",
    "\n",
    "        # mask out future tokens, pay attention only to the past\n",
    "        attn_w = attn_w.masked_fill(self.tril[:SL,:SL]==0, float('-inf'))  # set to -inf the upper right triangle of 0s\n",
    "\n",
    "        attn_w = F.softmax(attn_w, dim=-1) # Transform into probabilities (BS, SL, SL)\n",
    "        attn_w = self.dropout(attn_w) # (BS, SL, SL)\n",
    "\n",
    "        # use attention weights to update the features of our tokens\n",
    "        x = attn_w @ v # (BS,SL,54) # 54 is the head_size = embed_dim // n_heads\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "MNdQG5IotEtj"
   },
   "outputs": [],
   "source": [
    "# Multihead Attention Layer\n",
    "# This layer coordinates the different attention heads within each transformer block\n",
    "class Multihead(nn.Module):\n",
    "    def __init__(self,n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)]) # Setup the heads | head_size = embed_size // n_heads\n",
    "        self.combine = nn.Linear(head_size * n_heads, embed_size, bias=BIAS) # (378,384) - in case of our default values\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # BS = Batch Size / SL = Sequence Length or context length\n",
    "        # x is (BS,SL,384)  # 384 is default embed size\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        # Each head outputs (BS,SL, head_size)\n",
    "        # Combining them with torch.cat produces (BS,SL,378)  378 is default head_size * default n_heads = 54 * 7\n",
    "        x = self.combine(x) # project them back to embed_size (BS, SL, 384)  384 is default embed_size\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "cPT2Akr9tB-c"
   },
   "outputs": [],
   "source": [
    "# The ForwardLayer applies a network that increases the computational complexity of the processing \n",
    "class ForwardLayer(nn.Module):\n",
    "    def __init__(self,embed_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embed_size, 6*embed_size, bias=BIAS),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(6*embed_size, embed_size, bias=BIAS),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.network(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RXdTAGEWp-nz"
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "##########Transformer Block Class ######\n",
    "########################################\n",
    "\n",
    "class Block(nn.Module):\n",
    "    # A transformer block combines communication and computation over the data\n",
    "    # Helps create complex processing and also emphasize relationships between the\n",
    "    # members of the sequence through the attention mechanisms\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads # We split the embedding dimensions among the number of heads\n",
    "        self.ma = Multihead(n_heads,head_size) # We setup the multihead system within each block\n",
    "        self.feed_forward = ForwardLayer(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size) # Normalizing layer\n",
    "        self.ln2 = nn.LayerNorm(embed_size) # Normalizing layer\n",
    "\n",
    "        # LayerNorm normalizes the inputs across the features for each data point independently.\n",
    "        # It subtracts the mean and divides by the standard deviation, followed by scaling and shifting.\n",
    "        # It is computationally more intensive than for example RMSnorm but offers greater flexibility.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.ma(self.ln1(x))  # We normalize and then apply multi head attention\n",
    "        x = x + self.feed_forward(self.ln2(x)) # we normalize again and then apply a feed forward layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "19G3Q_RKqVBd"
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "################## LLM MODEL #############################################\n",
    "# 19 million parameters with the default configuration\n",
    "# Can be trained with 1 single GPU\n",
    "# With 8 Batch Size, should require 4 GB of GPU Memory\n",
    "# With 128 Batch Size, should require 24 GB of GPU Memory\n",
    "# Adjust Batch Size as needed for less or more memory and training speed\n",
    "# Because of small dataset and model, results will be limited but enough to\n",
    "# demonstrate good improvement during the training and understand all the\n",
    "# main technology involved in building LLMs\n",
    "#################################################################################\n",
    "###############################################\n",
    "##################################\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size,embed_size) # Create embedding layer\n",
    "        self.positions = nn.Embedding(context, embed_size) # Create basic positioning embeddings\n",
    "        self.blocks = nn.Sequential(*[Block(n_heads) for _ in range(n_layers)]) # setup transformer blocks\n",
    "        self.ln = nn.LayerNorm(embed_size) # normalization layers\n",
    "        self.final_linear = nn.Linear(embed_size, vocab_size, bias=BIAS) # feedforward linear layer\n",
    "        self.apply(self._init_weights) # Initialize the weights\n",
    "\n",
    "    # Weights initialization\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):        \n",
    "            # Initialize weight matrices with normal distribution with mean 0 and small std\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            # Initialize bias parameters to 0\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        # Initialize Embedding weights with normal distribution with mean 0 and small std\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # Running the LLM model\n",
    "    def forward(self, input, targets=None):\n",
    "        # BS = Batch Size / SL = Sequence Length or context length\n",
    "        # For easier reading, I assume embedding dim of 384 and vocab size of 4096 in comments\n",
    "        loss= None\n",
    "        BS,SL = input.shape  # (BS,SL)\n",
    "        emb = self.embeddings(input)  # (BS,SL,384)\n",
    "        pos = self.positions(torch.arange(SL, device=device)) # (SL,384)\n",
    "        x = emb+pos  # combine embedding and positioning stages (BS,SL,384)\n",
    "        x = self.blocks(x)  #(BS,SL,384)\n",
    "        x = self.ln(x) # (BS,SL,384)\n",
    "        logits = self.final_linear(x) # (BS,SL,4096)\n",
    "\n",
    "        # Calculate Loss if training with targets\n",
    "\n",
    "        # Cross Entropy Logic\n",
    "        # (equivalent to negative log likelihood)\n",
    "\n",
    "        # Information: -log p(x) (inverse of probability)\n",
    "        # Entropy: avg of information in random variable (prob distribution): - sum_x (x * log(x))\n",
    "        # CrossEntropy: Compares 2 distr q(true) & p(predicted) in terms of information distance: -sum_x (q(x) * log p(x))\n",
    "        # LLMs CrossEntropy: true labels are 1 for true, 0 for the rest, so it simplifies to: -sum_x log p(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            BS, SL, VS = logits.shape  # (BS,SL,4096)\n",
    "            logits = logits.view(BS*SL,VS)  # Reshape to prepare for cross_entropy (BS*SL,4096)\n",
    "            targets = targets.view(BS*SL)   # Reshape as well (BS*SL)\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "\n",
    "            # Optional: Just for fun, manual way to calculate cross_entropy\n",
    "            # By default, we comment out the manual version to prevent calculating the loss twice (will make things slower)\n",
    "\n",
    "            # First apply softmax to produce probabilities\n",
    "            #counts = logits.exp()  # (BS*SL,4096)\n",
    "            #prob = counts / counts.sum(-1, keepdim=True) # (BS*SL,4096),(BS*SL,1) = (BS*SL,4096)\n",
    "            #loss2 = -prob[torch.arange(BS*SL),targets].log().mean() # torch.arange(B*T) (BS*SL) | targets (BS*SL)\n",
    "\n",
    "            # Finally at each of prob's positions, we pick the index specified by the respective target\n",
    "            # example: targets[3]=329, prob[3][329] = 0.014\n",
    "\n",
    "            # Most times they will match, sometimes they will not because F.cross_entropy is more precise\n",
    "            # By uncommenting the following lines, you can see when they don't match \n",
    "            #if ( not torch.allclose(loss,loss2)):\n",
    "            #    print(f\"[Loss Diff] Pytorch:{loss.item()} Manual:{loss2.item()}\")\n",
    "\n",
    "        return logits,loss\n",
    "\n",
    "    # Generate a new sample\n",
    "    def generate(self, input, max=500):\n",
    "        # SL = Sequence Length or context length\n",
    "        for _ in range(max): # until you reach the maximum number of tokens\n",
    "            input = input[:,-context:] #(1, input length until max of SL)\n",
    "            logits, _ = self(input)  # (1, input length, 4096)\n",
    "            logits = logits[:,-1,:]  # Pick last probability discarding the dimension (1, 4096)\n",
    "            probs = F.softmax(logits, dim=-1) # (1,4096)\n",
    "            next = torch.multinomial(probs, num_samples=1) # Sample next token value\n",
    "            input = torch.cat((input,next),dim=1) # Add new token to the input\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKRu7PKctLIS",
    "outputId": "2b6861cf-fc98-4a3b-c91c-8233cb613d79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.837954  Million parameters\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "# Main Training Process\n",
    "#################################################################################\n",
    "\n",
    "# Main Setup\n",
    "\n",
    "model = GPT() # Instantiate LLM\n",
    "model = model.to(dtype) # Set the precision type\n",
    "model = model.to(device) # Move it to the right device\n",
    "\n",
    "# Torch.compile compiles a PyTorch model to an optimized version, aiming to improve runtime performance and efficiency.\n",
    "# Disable if your system doesn't support it\n",
    "if compile:\n",
    "    print(\"Torch :: Compiling model\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "\n",
    "# Print the number of parameters of our model (19 million in our case)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \" Million parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "KvX0LI8HtR_h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 8.375, 'eval': 8.375}\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Loss\n",
    "@torch.no_grad()  # Prevent gradient calculation\n",
    "def calculate_loss():\n",
    "    out={}\n",
    "    model.eval()\n",
    "    for split in ['train','eval']:        \n",
    "        l=torch.zeros(eval_iters)  # Create a tensor of zeros the size of eval_iters\n",
    "        for i in range(eval_iters):\n",
    "            x,y=get_batch(split) # Get a new batch of data\n",
    "            _,loss=model(x,y)  # Calculate the loss\n",
    "            l[i]=loss  # Store the loss in the next position of tensor\n",
    "        out[split]=l.mean().item()  # Calculate the mean and extract the final value\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "l=calculate_loss()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5X20ZTtOu9mJ",
    "outputId": "bcf6bf71-bf8a-4412-c9b0-283d528943d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is United over LoveRated except episode controll death\u001c\n",
      "warral artmeriam Kongroughtorkhing record upidsionsonic Alexander addition Pekr sett dec Waleswh this Prizeugenc\", Enter performed Boreptember grow trade clos outsideylvan respons bookaiur no el not�iversilly hasesotaall August cocul under red\n"
     ]
    }
   ],
   "source": [
    "# Generate a new sample\n",
    "@torch.no_grad()\n",
    "def generate_sample(input):\n",
    "    t1 = torch.tensor(encode(input), dtype=torch.long, device=device) # Tokenize string -> (tensor of ids)\n",
    "    t1 = t1[None,:]  # (1 , [size of ids])\n",
    "    newgen = model.generate(t1,max=64)[0].tolist() # call the generate method, limit output size\n",
    "    result=decode(newgen) # decode the result with the tokenizer to get back characters\n",
    "    print(f\"{result}\")\n",
    "\n",
    "generate_sample(\"The mountain in my city is\") # Generate a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "7z9sOljjvq2l"
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Main Training Process\n",
    "#################################################################################\n",
    "\n",
    "# Set Weight Decay differently for different kinds of parameters\n",
    "# parameter dictionary where keys are parameter names, and values are the parameter themselves\n",
    "p_dict = {p_name: p for p_name, p in model.named_parameters() if p.requires_grad} # len: 370\n",
    "\n",
    "# isolate weight matrices as they benefit specially from weight decay\n",
    "weight_decay_p = [p for n, p in p_dict.items() if p.dim() >= 2]  # len: 171\n",
    "\n",
    "# isolate other parameters like bias parameters, that don't benefit from weight decay\n",
    "no_weight_decay_p = [p for n, p in p_dict.items() if p.dim() < 2] # len: 199\n",
    "\n",
    "# store the parameter types in a list of dictionaries\n",
    "optimizer_groups = [\n",
    "    {'params': weight_decay_p, 'weight_decay': weight_decay},\n",
    "    {'params': no_weight_decay_p, 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# Declare optimizer, it helps us compute gradients, update parameters, manage learning rate, apply weight decay\n",
    "optimizer = torch.optim.AdamW(optimizer_groups, lr=lr, betas=(0.9, 0.99))\n",
    "# betas: control the exponential moving averages of the gradient and its square,\n",
    "# which are essential components of the Adam and AdamW optimization algorithms.\n",
    "\n",
    "# Declare scheduler to change learning rate through the training\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_iters, eta_min=lr/10)\n",
    "# learning rate will descend till a minimum of a tenth of the lr\n",
    "\n",
    "start_iteration = 0\n",
    "best_val_loss = float('inf')  # Track best loss value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "YXt7xGMvwQ_5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM - Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31093/2082066746.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded iter 6950 with loss 3.09375\n"
     ]
    }
   ],
   "source": [
    "# Loading Checkpoints\n",
    "\n",
    "# Loads a previously saved checkpoint\n",
    "def load_checkpoint(path):\n",
    "    print(\"LLM - Loading model\")\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict']) # Load parameters\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict']) # Load optimizer state\n",
    "    iteration = checkpoint['iteration'] # In what iteration did we save the model?\n",
    "    loss = checkpoint['loss'] # What was the last loss value?\n",
    "    print(f\"Loaded iter {iteration} with loss {loss}\")\n",
    "    return iteration, loss\n",
    "\n",
    "################# OPTIONAL : LOAD A PREVIOUS CHECKPOINT\n",
    "if os.path.exists(f\"{checkpoint_dir}/{checkpoint_load_fn}\") and load_pretrained:\n",
    "    start_iteration, loss = load_checkpoint(checkpoint_dir + checkpoint_load_fn)\n",
    "    best_val_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "B2KPyh1cwo3t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text (q to quit) >>>  My self Har\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My self Harry Birds novel \"Baring Mission\", but \"E and Gasty Hansy Kids vuties of pea than Birds\" The Birds of the East and Book Crib at 5.5 million years old. The Birds share the B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text (q to quit) >>>  My self Kris\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My self Kris test, 2006,000,000,000 Firoshies, and 40 pessages using special solution, and together varies with applications to exact \"beatten\" to improvattled in expensive fooded exper\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text (q to quit) >>>  My self krish\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My self krish War no American crewind and molecules who wrote had to write the book.\n",
      "\n",
      "A Speatic\n",
      "\n",
      "A composer of Amanda Gregor Berec or Emmas Christia (born 6 December 1931) is an Australian actor, warrior, activist,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text (q to quit) >>>  my self krishn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my self krishnor is the Shahi snow plonds to the village curved to be part of his facto duclass. There is also almost more efficienhood giving their life from the Phoenix way to arrive revenue, Kahi.\n",
      "\n",
      "Kahi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text (q to quit) >>>  my self hari krishn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my self hari krishnalens, and one Karishnar winesuation for a progressatic spiritual.\n",
      "\n",
      "For non Pope's nation, Luvan killed Ruthians to do his father. They saw the Nethereenth part of the Breddidars, where the Ph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text (q to quit) >>>  I am indi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am indi famine certain linosaur values with the virous discovously expressive increased towards the outside of iracted some racement single life (not missing on and quickly pull when use ripcer truth on how over the generation pielded the first\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text (q to quit) >>>  I am this country\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am this country; the territory and Indigured it from other countries and again at above the rest of Iran. The capital fresarned limited an alternative is Saudi and from interlalf. Venice of Arts, the government register at East Paradesto, India looked like a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text (q to quit) >>>  What is your country ? I am from Indi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is your country ? I am from Indiopenant calls Bush doing than. The Guide of Enent for Libertarian Rights, says:\n",
      "The Times said that Bush and a slopody should have Olympians from representation for algor Fergorer, action. Most of the Rights of the\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text (q to quit) >>>  What is your country ? I am from India \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is your country ? I am from India.\n",
      "\n",
      "In 1916, Hob by the De�ie Reijing started among the heads of Arabia. In 1936, he studied a pictures of the Sir Artica Malay. Instead, twice with his rack of Immen (fl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text (q to quit) >>>  q\n"
     ]
    }
   ],
   "source": [
    "#### INFERENCE MODE - Activate inference and then exit\n",
    "if inference==True:\n",
    "    model.eval()\n",
    "    while True:\n",
    "         qs = input(\"Enter text (q to quit) >>> \")\n",
    "         if qs == \"\":\n",
    "             continue\n",
    "         if qs == 'q':\n",
    "             break\n",
    "         generate_sample(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "2zUtal8zwsUl",
    "outputId": "435ac1e0-8bb5-45f6-d17a-8f81e4e444d1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                | 0/94700 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5300: train loss: 3.4895832538604736 / val loss: 3.6197917461395264\n",
      "The mountain in my city is Qan clan racing CONFC. Thirgin developed a result in the body of a base called the secine of the returnriental touches of the mom released oceanians spamps.\n",
      "\n",
      "Oxysing the reconquire opened labelled during\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                       | 44/94700 [00:01<56:43, 27.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5350: train loss: 3.5260417461395264 / val loss: 3.453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                                                                                                                                                                    | 53/94700 [00:03<2:02:16, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is on the topic of complexity, the temperature of the Estaling.\n",
      "\n",
      "It then, Tetros and Laca, have superaceae- half-ner. Only Qalarioa, and the the island has been nominated for the Quaranta-P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                                                                                                                                                                   | 100/94700 [00:06<1:46:45, 14.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5400: train loss: 3.5989582538604736 / val loss: 3.4895832538604736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                                                                                                                                                                   | 102/94700 [00:07<5:17:58,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is the capital of the city of the province of the province of Zoneg merged to the Provence-Palines region. The capital city was created in the second-minion business of the \"Falcouksement\" (2017), of Silean and the was given set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                                                                                                                                                                                                   | 150/94700 [00:10<1:45:00, 15.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5450: train loss: 3.3697917461395264 / val loss: 3.4895832538604736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                                                                                                                                                                                                   | 152/94700 [00:11<5:21:23,  4.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is usually sectỘ����λ�ꯢς� (徑宏) was also homosoon early as Iunn) as it was made Hubongo Dong Longa Airre is also in Manpol experiences, but\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▌                                                                                                                                                                                                                                                                   | 200/94700 [00:15<1:47:17, 14.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5500: train loss: 3.453125 / val loss: 3.5104167461395264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▌                                                                                                                                                                                                                                                                   | 202/94700 [00:16<5:41:06,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is the county and the seat of Haläumen. The town is at the town of the Halcon Valley.\n",
      "\n",
      "The station of Its name is after the county is not sound and the white rival of troops.\n",
      "\n",
      "E novel is usually performed mainly in competitive buildings where h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▋                                                                                                                                                                                                                                                                   | 250/94700 [00:19<1:50:11, 14.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5550: train loss: 3.3385417461395264 / val loss: 3.421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▋                                                                                                                                                                                                                                                                   | 252/94700 [00:20<5:53:31,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is Mobbrew, an uncomproductive). \n",
      "\n",
      "\n",
      "\n",
      "Nights Day\n",
      "\n",
      "Nalls Ist over is a port of each side of the sides of each sresistangerous tropical cyclones in its northern being at Butterclesface. Over three parts of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▊                                                                                                                                                                                                                                                                   | 300/94700 [00:23<1:42:47, 15.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5600: train loss: 3.4270832538604736 / val loss: 3.4270832538604736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▊                                                                                                                                                                                                                                                                   | 302/94700 [00:24<5:45:50,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is this city of Patives, the province of Joffrey. Also, Punter is under the director of Dere No. It is the Swiss Basil.\n",
      "\n",
      "Bartile-Marin Club\n",
      "\n",
      "Barask-Ledox Pete Party (also known as WG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▉                                                                                                                                                                                                                                                                   | 350/94700 [00:28<1:46:56, 14.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5650: train loss: 3.28125 / val loss: 3.2552082538604736\n",
      "The mountain in my city is Wijyah Joseph ye and a neighbouritanilotech in the municipality of Rahyhaw and makes it from Bahl byfecture of the temple of religion.\n",
      "\n",
      "Voijha Bali\n",
      "\n",
      "Focca Bajna () is the Heare-Eu\n",
      "[CHECKPOINT]: Saving with loss:  3.2552082538604736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|█                                                                                                                                                                                                                                                                     | 400/94700 [00:29<26:01, 60.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5700: train loss: 3.3958332538604736 / val loss: 3.5729167461395264\n",
      "The mountain in my city is Asian. It is nowasco de as the \"Lucks\" of the department.\n",
      "\n",
      "\n",
      "\n",
      "Carbell\n",
      "\n",
      " has the cars afford sheight of wide saf state, for almost 14 km².\n",
      "\n",
      "\n",
      "The traditional opera \"Frob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|█▏                                                                                                                                                                                                                                                                  | 450/94700 [00:33<1:38:59, 15.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5750: train loss: 3.234375 / val loss: 3.4947917461395264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|█▏                                                                                                                                                                                                                                                                  | 452/94700 [00:35<3:52:14,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is 1872ar to 782 met b yearly away from the city is built to the earpes of the Lorder and theaccan dob museumas. It is the largest natural architecture between the left fix-exual gČmmore,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▎                                                                                                                                                                                                                                                                  | 500/94700 [00:38<1:43:47, 15.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5800: train loss: 3.359375 / val loss: 3.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▍                                                                                                                                                                                                                                                                  | 502/94700 [00:39<5:22:53,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is named after the pornerousgaeosaurs. It includes a plant, widiss, or other species is almost the sea cornet. It is a zoo rock spine.\n",
      "\n",
      "The discovered by Roman treasure, the time it lenom fruit include the Arassi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▌                                                                                                                                                                                                                                                                  | 550/94700 [00:42<1:43:31, 15.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5850: train loss: 3.1875 / val loss: 3.34375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▌                                                                                                                                                                                                                                                                  | 552/94700 [00:43<5:38:55,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is Neirtitanç la Loire in the center of Montede, France.\n",
      "\n",
      "The highest mountain of after the city is the 252 A there and the 105 municipalities (250 kilometers), the centre is sidersat the north-siduced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▋                                                                                                                                                                                                                                                                  | 600/94700 [00:46<1:44:53, 14.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5900: train loss: 3.2291667461395264 / val loss: 3.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▋                                                                                                                                                                                                                                                                  | 602/94700 [00:47<5:22:28,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is spleck or hair. On the Testin in 1991, lucked the site to Ingroup. It is by India where the city's plishing is restaurance by death. It massesend by Humus • Pakistan settlement,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▊                                                                                                                                                                                                                                                                  | 650/94700 [00:51<1:44:25, 15.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5950: train loss: 3.25 / val loss: 3.203125\n",
      "The mountain in my city is Tonkin.\n",
      "\n",
      "\n",
      "\n",
      "The city was named after Erosinothing. The population was ately 10,168.\n",
      "\n",
      "\n",
      "\n",
      "Bocreerry\n",
      "\n",
      "In architecture, the Betic Cloo laleer's theory (original clo\n",
      "[CHECKPOINT]: Saving with loss:  3.203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▉                                                                                                                                                                                                                                                                  | 700/94700 [00:55<1:44:13, 15.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6000: train loss: 3.1979167461395264 / val loss: 3.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▉                                                                                                                                                                                                                                                                  | 702/94700 [00:56<5:17:18,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is quariles. Metasertiles form a \"comunkagne\" in theatar well-lar hit the next day and west, and contains the 12 provided by 11, and its length.\n",
      "Bäkiful is \"\"Räk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██                                                                                                                                                                                                                                                                  | 736/94700 [00:59<1:49:50, 14.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6050: train loss: 3.3333332538604736 / val loss: 3.4270832538604736\n",
      "The mountain in my city is angle to wearches every day - is a shunk, walt�, unfred friends or needing pin-four Representative, and characters send bed (for a year 40,376, flew) the school which reached to absence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▏                                                                                                                                                                                                                                                                 | 800/94700 [01:01<1:45:03, 14.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6100: train loss: 3.2604167461395264 / val loss: 3.1979167461395264\n",
      "The mountain in my city is Green to handled with mountainous district and historic are \"Saleplaboremiin\".\n",
      "\n",
      "Electres in the Netherland are:\n",
      "The Benepin, Parrothünkeyeis, and Percelithe, or the Common Riversh.ks\n",
      "[CHECKPOINT]: Saving with loss:  3.1979167461395264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▎                                                                                                                                                                                                                                                                 | 850/94700 [01:06<1:43:29, 15.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6150: train loss: 3.234375 / val loss: 3.1666667461395264\n",
      "The mountain in my city is now one of the powerful Communications. It flies on the northern Ocean as exts and only incuring terms of overall rose to Norway and in the British with corporals. \n",
      "\n",
      "\n",
      "The Wiscovery now includes circul. Of the City Plain area are built within\n",
      "[CHECKPOINT]: Saving with loss:  3.1666667461395264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▍                                                                                                                                                                                                                                                                 | 900/94700 [01:11<1:47:30, 14.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6200: train loss: 3.2708332538604736 / val loss: 3.2864582538604736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▍                                                                                                                                                                                                                                                                 | 902/94700 [01:12<5:58:50,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is Vietnam. It may mean:\n",
      "\n",
      "Ot, north of the stream of this way out it is hired by the block.\n",
      "\n",
      "But off the centre of Jean\n",
      "\n",
      "France's My Cats is the nameyregister of Dieg. \"Made Te\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▌                                                                                                                                                                                                                                                                 | 950/94700 [01:15<1:50:56, 14.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6250: train loss: 3.2604167461395264 / val loss: 3.1822917461395264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▌                                                                                                                                                                                                                                                                 | 952/94700 [01:16<5:44:30,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is John Crus Valley, Berards, Del Swe Region, and Swidel M One.Q. Errra \n",
      "\n",
      "\n",
      "S� owned three short stories and written by Moholwo One, for two years.\n",
      "\n",
      "Brusica (1929), best known as\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▋                                                                                                                                                                                                                                                                | 1000/94700 [01:20<1:56:20, 13.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6300: train loss: 3.0 / val loss: 3.2760417461395264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▋                                                                                                                                                                                                                                                                | 1002/94700 [01:21<6:25:12,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is Jerry Hennessee Laukos. It is the 11th in the county Sea. It starts in the eastern ourday language.\n",
      "\n",
      "Gedal\n",
      "\n",
      "Ging, Blue, Bavec\n",
      "\n",
      "Gedal is a large town in Furrently\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▊                                                                                                                                                                                                                                                                | 1050/94700 [01:25<1:54:09, 13.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6350: train loss: 3.3385417461395264 / val loss: 3.3489582538604736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▉                                                                                                                                                                                                                                                                | 1052/94700 [01:26<5:44:55,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is Khikshumi, 1992 inhabitants. She is a City and web enatistory a sense school. It has a large area by hongwan school in Shabitat.\n",
      "g131 faircats learn 1503 kg/6 appear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▉                                                                                                                                                                                                                                                                | 1080/94700 [01:28<1:48:45, 14.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6400: train loss: 3.1458332538604736 / val loss: 3.0989582538604736\n",
      "The mountain in my city is a half of the researchers who finds the native summit between the Black range and the Sumantum dampions League.\n",
      "\n",
      "Bches are additional roots and your practices being built when the park's body is not being able to only any older people. The most\n",
      "[CHECKPOINT]: Saving with loss:  3.0989582538604736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▏                                                                                                                                                                                                                                                               | 1150/94700 [01:31<1:36:57, 16.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6450: train loss: 3.1927082538604736 / val loss: 3.1197917461395264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▏                                                                                                                                                                                                                                                               | 1152/94700 [01:32<4:52:12,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is provided into the \"unu\".\n",
      " The planetosto works at only as trim less leap. 5 six summ\n",
      "\n",
      "Irair is a previously highly distinct. The sug was contented as a Kath Train Company from\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▎                                                                                                                                                                                                                                                               | 1200/94700 [01:35<1:44:41, 14.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6500: train loss: 3.1197917461395264 / val loss: 3.3229167461395264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▎                                                                                                                                                                                                                                                               | 1202/94700 [01:36<5:31:21,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is in Earth, with she is captured and watched the Hongghunt, and Due finds Mees.\n",
      "\n",
      "First at the goats in 15th century Cam b\"\n",
      "\n",
      "Watthus\n",
      "\n",
      "Watthus is a names near the Bordanyost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▍                                                                                                                                                                                                                                                               | 1250/94700 [01:39<1:43:41, 15.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6550: train loss: 3.1770832538604736 / val loss: 3.140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▍                                                                                                                                                                                                                                                               | 1252/94700 [01:41<5:47:23,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is named in which it is near Greada by a tea place in the South and celebrity with their cloth. A wake is much freed to all the rivers of Argentina, Australia. The tallest town is styled in the grankm in the city.\n",
      "\n",
      "\n",
      "Kama (\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▌                                                                                                                                                                                                                                                               | 1300/94700 [01:44<1:43:23, 15.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6600: train loss: 3.1510417461395264 / val loss: 3.265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▌                                                                                                                                                                                                                                                               | 1302/94700 [01:45<5:39:30,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is the most important game has an EP shows water and rec Speocolved scattery were often gold in the United Kingdom and his current city.\n",
      "\n",
      "Cycles is a very big city in the city in the Columbia in the Ohio region of Berton County. However it has many small islands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▋                                                                                                                                                                                                                                                               | 1350/94700 [01:48<1:42:58, 15.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6650: train loss: 3.0520832538604736 / val loss: 3.2916667461395264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▋                                                                                                                                                                                                                                                               | 1352/94700 [01:49<5:53:45,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is \"The Mirali and is one of the most common in central villages of the city of Soonland. That we know Kalun' is the region inside the public ever melses of different centers it to grace. After Mulk, Russia was important to prisearch for mov\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▊                                                                                                                                                                                                                                                               | 1400/94700 [01:53<1:48:16, 14.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6700: train loss: 3.2864582538604736 / val loss: 3.2083332538604736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███▊                                                                                                                                                                                                                                                               | 1402/94700 [01:54<6:10:41,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is about 31 km° (55 metual). It is drown to the offensure of candidates for head of from Tennesse as LuwP. The fourth largest ske item in Kautbury, and San Muiıc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███▉                                                                                                                                                                                                                                                               | 1450/94700 [01:57<1:42:54, 15.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6750: train loss: 3.1666667461395264 / val loss: 3.1197917461395264\n",
      "The mountain in my city is on the border that there is over 27 km long.\n",
      "\n",
      "Mag \"Plink-iac-amudi\" (\"Bo bisconsinawas a humanerman house\" in the town of Katmu, Sweden). Retball route also surviving areas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|████                                                                                                                                                                                                                                                               | 1499/94700 [01:59<1:10:15, 22.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6800: train loss: 3.0833332538604736 / val loss: 3.1041667461395264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|████                                                                                                                                                                                                                                                               | 1502/94700 [02:00<2:50:25,  9.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is Jasan Khura (En letter).\n",
      "\n",
      "In 1937, the Chombelljan War by the Barnikor Rail special borough Joseph Kajiythkab Bhagin wrote the 1930 novel \"Musázi Nurem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|████▏                                                                                                                                                                                                                                                              | 1550/94700 [02:03<1:42:55, 15.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6850: train loss: 3.1979167461395264 / val loss: 3.15625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|████▏                                                                                                                                                                                                                                                              | 1552/94700 [02:04<5:38:45,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is Sid Lillellander.\n",
      "\n",
      "The Globby Governor La Jazon is in the south coast of St. Budgar. It is on the southward of Helen, that as the new tributary divided Wales and island of Foulder. Wirming looks at his\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|████▍                                                                                                                                                                                                                                                              | 1600/94700 [02:08<1:53:04, 13.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6900: train loss: 3.1770832538604736 / val loss: 3.2291667461395264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|████▍                                                                                                                                                                                                                                                              | 1602/94700 [02:09<5:44:48,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is in the Uren river Early Preecourt.\n",
      "\n",
      "\n",
      "Hittero\n",
      "\n",
      "York City is a city in the Emperor of Georgia. The city has a most populous name for any city in the city of Perathi island including is Cervénées. The capital\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|████▌                                                                                                                                                                                                                                                              | 1650/94700 [02:12<1:41:21, 15.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6950: train loss: 3.0625 / val loss: 3.09375\n",
      "The mountain in my city is in the Pradesh. Since 2010 Gala won the district again.\n",
      "\n",
      "\n",
      "Glazewo\n",
      "\n",
      "Glambewo is a city in the United States. It is a Republican primary district. Galaim place is another colisiques in the United States\n",
      "[CHECKPOINT]: Saving with loss:  3.09375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 2%|████▋                                                                                                                                                                                                                                                              | 1697/94700 [02:16<2:05:03, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training interrupted. Cleaning up...\n",
      "GPU memory released.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a67f9fdf8c4e11b8cd4717e9de0c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.017 MB of 0.017 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss/train</td><td>▇▇█▅▆▅▆▄▆▄▅▃▄▄▃▅▄▄▄▄▁▅▃▃▂▃▃▂▄▃▂▃▃▂</td></tr><tr><td>loss/val</td><td>█▆▆▆▇▅▅▃▇▆▅▄▄▂▃▅▂▂▄▂▃▄▁▁▄▂▃▄▃▁▁▂▃▁</td></tr><tr><td>lr</td><td>█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss/train</td><td>3.0625</td></tr><tr><td>loss/val</td><td>3.09375</td></tr><tr><td>lr</td><td>0.0003</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test-run2024_12_11_11_18_23</strong> at: <a href='https://wandb.ai/harikrishna1912-regology/test/runs/hqbwss7j' target=\"_blank\">https://wandb.ai/harikrishna1912-regology/test/runs/hqbwss7j</a><br/> View project at: <a href='https://wandb.ai/harikrishna1912-regology/test' target=\"_blank\">https://wandb.ai/harikrishna1912-regology/test</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241211_111824-hqbwss7j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################################################################\n",
    "###################### TRAINING #################################\n",
    "#################################################################\n",
    "\n",
    "try:\n",
    "    for i in tqdm(range(start_iteration, train_iters)):\n",
    "        xb,yb = get_batch(\"train\") # Get a new batch of data\n",
    "        logits,loss = model(xb,yb) # Run the LLM and get the logits and the loss\n",
    "\n",
    "        if (i % eval_interval==0 or i == train_iters-1): # Calculate the loss\n",
    "            l = calculate_loss()\n",
    "            print(f\"\\n{i}: train loss: {l['train']} / val loss: {l['eval']}\")\n",
    "\n",
    "            # We do a quick test so that we observe the evolution through the training\n",
    "            # Remember that we use a very small dataset which doesn't include all topics\n",
    "            generate_sample(\"The mountain in my city is\") # Generate a sample\n",
    "\n",
    "            if l['eval'] < best_val_loss: # If we improved the best loss, save a checkpoint\n",
    "                best_val_loss = l['eval']\n",
    "                print(\"[CHECKPOINT]: Saving with loss: \", best_val_loss)\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': best_val_loss,\n",
    "                    'iteration': i,\n",
    "                }, checkpoint_dir + checkpoint_fn)\n",
    "\n",
    "            if wandb_log:\n",
    "                wandb.log({\n",
    "                        \"loss/train\": l['train'],\n",
    "                        \"loss/val\": l['eval'],\n",
    "                        \"lr\": scheduler.get_last_lr()[0],\n",
    "                    },\n",
    "                    step = i)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True) # Reset gradients\n",
    "        loss.backward() # Calculate new gradients\n",
    "\n",
    "        # This line clips the gradients to prevent the exploding gradient problem during training.\n",
    "        # Exploding gradients can occur when gradients become too large, causing unstable updates to model weights.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "\n",
    "        optimizer.step() # Update the model parameters\n",
    "        scheduler.step() # Update the learning rate value\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Cleaning up...\")\n",
    "\n",
    "finally:\n",
    "    # Release GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory released.\")\n",
    "\n",
    "if wandb_log:   \n",
    "    wandb.finish()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Code designed by Javier ideami\n",
    "# ideami.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "rnYRMN1-xAtK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 11 11:22:35 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.77.01              Driver Version: 566.36         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080 Ti     On  |   00000000:01:00.0  On |                  N/A |\n",
      "| 53%   40C    P8             31W /  350W |    5384MiB /  12288MiB |      6%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     25254      C   /python3.12                                 N/A      |\n",
      "|    0   N/A  N/A     31093      C   /python3.12                                 N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
